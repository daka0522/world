{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.WIDTH = 4\n",
    "        self.HEIGHT = 4\n",
    "        self.reward_map = np.zeros((self.HEIGHT, self.WIDTH), dtype=int)\n",
    "        self.reward_map[0, 3] = 1 #reward\n",
    "        self.reward_map[3, 3] = -1 #punish\n",
    "    \n",
    "    def next_step(self, state):\n",
    "        if 0 <= state[0] < 4 and 0 <= state[1] < 4:\n",
    "            reward = self.reward_map[state]\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward \n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env: Env):\n",
    "        self.state = np.array((3, 0)) # initial location, current location\n",
    "        self.memory = defaultdict(dict) # size: (H, W, Number of Actions) H, W: Map size\n",
    "        self.initial_action_rewards = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "        self.actions = [1, 2, 3, 4] # up, right, down, left\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.new_memory = np.zeros((env.HEIGHT, env.WIDTH, len(self.actions)))\n",
    "\n",
    "\n",
    "\n",
    "    def expect2(self, state: tuple[int, int]):\n",
    "        history_of_state = self.new_memory[state[0], state[1]]\n",
    "        return history_of_state\n",
    "    \n",
    "    def best_action2(self, history_of_state):\n",
    "        arg = np.argmax(history_of_state)\n",
    "        return self.actions[arg]\n",
    "        \n",
    "    \n",
    "\n",
    "    def expect(self, state: tuple[int, int]):\n",
    "        if len(self.memory[tuple(state)]) == 0: # if it's initial then give initial value.\n",
    "            self.memory[tuple(state)] = self.initial_action_rewards\n",
    "        history_of_state = self.memory[tuple(state)]\n",
    "        return history_of_state\n",
    "\n",
    "    def best_action(self, state, history_of_state):\n",
    "    \n",
    "        # if len(history_of_state) < 4:\n",
    "        #     action = np.random.choice(self.actions)\n",
    "        # else: \n",
    "        arg = np.argmax(list(history_of_state.values()))\n",
    "        # action_rewards = list(self.memory[tuple(state)].keys())\n",
    "        action = self.actions[arg]\n",
    "        print(f\"history: {history_of_state}, Arg: {arg}, Action: {action}\")\n",
    "\n",
    "        # print(f\"action: {action}\")\n",
    "        return action \n",
    "        \n",
    "\n",
    "    def state_transition(self, action):\n",
    "        if action == 1:\n",
    "            new_state = self.state + np.array([-1, 0])\n",
    "        elif action == 2:\n",
    "            new_state = self.state + np.array([0, 1])\n",
    "        elif action == 3:\n",
    "            new_state = self.state + np.array([1, 0])\n",
    "        elif action == 4:\n",
    "            new_state = self.state + np.array([0, -1])\n",
    "        else:\n",
    "            raise Exception(\"Unvalid action\")\n",
    "        return new_state\n",
    "\n",
    "    def move_to_state(self, new_state):\n",
    "        if 0 <= new_state[0] < 4 and 0 <= new_state[1] < 4:\n",
    "            return new_state \n",
    "        else:\n",
    "            return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.new_memory[0, 0]\n",
    "agent.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actions.index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.new_memory[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = agent.expect2(agent.state)\n",
    "hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = agent.best_action2(hist)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = agent.state_transition(action)\n",
    "new_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = env.next_step(tuple(new_state))\n",
    "reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_to_new_state = agent.move_to_state(new_state)\n",
    "move_to_new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.expect2(agent.state)\n",
    "action = agent.best_action2(history)\n",
    "new_state = agent.state_transition(action)\n",
    "reward = env.next_step(tuple(new_state))\n",
    "agent.new_memory[tuple(agent.state)][action] +=  reward\n",
    "\n",
    "move_to_new_state = agent.move_to_state(new_state)\n",
    "\n",
    "agent.state = move_to_new_state\n",
    "agent.reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m = np.zeros((4,4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[(2, 3)] = [0, 0, -20, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0, 3][1] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_map[3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(m[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.,   0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there's same values while find argmax, and then want to pick randoms.\n",
    "\n",
    "np.unique(m[2,3], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0., -10.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0., -20.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  1., -10.,   0.,   0.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2280029074692811"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {0:0, 1:0, 2:0, 3:0}\n",
    "for i in range(1000):\n",
    "    result = np.random.randint(0, 4)\n",
    "    score[result] += 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 264, 1: 250, 2: 254, 3: 232}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
